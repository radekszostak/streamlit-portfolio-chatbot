{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('docs')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=st.secrets['OPENAI_API_KEY'])\n",
    "vectordb = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_docs = retriever.get_relevant_documents(\"What University has Radek finished?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_chat_history = lambda messages: '\\n'.join([f'{m[\"role\"]}: {m[\"content\"]}' for m in messages])\n",
    "format_additional_information = lambda documents: \"\\n\\n\".join([str(item.metadata) + \"\\n\" + item.page_content for item in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'format_chat_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m user_message \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYou: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m messages\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: user_message})\n\u001b[1;32m---> 17\u001b[0m chat_history \u001b[39m=\u001b[39m format_chat_history(messages)\n\u001b[0;32m     18\u001b[0m additional_information \u001b[39m=\u001b[39m format_additional_information(retriever\u001b[39m.\u001b[39mget_relevant_documents(chat_history))\n\u001b[0;32m     20\u001b[0m response \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'format_chat_history' is not defined"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are a chatbot that helps people to get information about Radek Szostak. Radek is a data scientist and a machine learning engineer. He is also a co-founder of the LangChain project. He is a very friendly and helpful person. He is always ready to help people. He is also a very good teacher. He is always ready to teach people. He is also a very good mentor. He is always ready to mentor. You have access to chat history in order to keep context of the conversation. You are also provided with additional information where you can search for answers.\n",
    "\n",
    "% start additional information %\n",
    "{additional_information}\n",
    "% end additional information %\n",
    "\n",
    "% chat history %\n",
    "{chat_history}\n",
    "assisstant:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=['chat_history'], template=template)\n",
    "client = OpenAI(model=\"gpt-3.5-turbo-instruct\", api_key=st.secrets[\"OPENAI_API_KEY\"], streaming=True)\n",
    "messages = []\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    chat_history = format_chat_history(messages)\n",
    "    additional_information = format_additional_information(retriever.get_relevant_documents(chat_history))\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in client.stream(prompt.format(chat_history=chat_history, additional_information=additional_information)):\n",
    "        response += chunk\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm = ChatOpenAI(),\n",
    "    retriever = vectordb.as_retriever(),\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "university teacher and member of an international research group.\n",
      "\n",
      "Interests\n",
      "\n",
      "Python\n",
      "\n",
      "environments. Nearly three years of experience as a software developer in a commercial company.\n",
      "Human: Who are you?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who are you?',\n",
       " 'chat_history': [],\n",
       " 'answer': 'I am an AI assistant designed to provide information and answer questions. I am not a specific person, but I can assist with a wide range of topics. Is there something specific you would like to know or discuss?'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke({'question': 'Who are you?', 'chat_history': []})#, 'chat_history': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'chat_history'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mWhat was the previous question?\u001b[39;49m\u001b[39m'\u001b[39;49m})\u001b[39m#, 'chat_history': []})\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\48794\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:138\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m include_run_info \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39minclude_run_info\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    136\u001b[0m return_only_outputs \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn_only_outputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 138\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_inputs(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    139\u001b[0m callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39mconfigure(\n\u001b[0;32m    140\u001b[0m     callbacks,\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata,\n\u001b[0;32m    147\u001b[0m )\n\u001b[0;32m    148\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\48794\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:475\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    473\u001b[0m     external_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mload_memory_variables(inputs)\n\u001b[0;32m    474\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexternal_context)\n\u001b[1;32m--> 475\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_inputs(inputs)\n\u001b[0;32m    476\u001b[0m \u001b[39mreturn\u001b[39;00m inputs\n",
      "File \u001b[1;32mc:\\Users\\48794\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:264\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m missing_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_keys)\u001b[39m.\u001b[39mdifference(inputs)\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m missing_keys:\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing some input keys: \u001b[39m\u001b[39m{\u001b[39;00mmissing_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Missing some input keys: {'chat_history'}"
     ]
    }
   ],
   "source": [
    "qa_chain.invoke({'question': 'What was the previous question?'})#, 'chat_history': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "template = \"\"\"\n",
    "You're a chatbot representing a talented individual looking to share insights with recruiters. Start by introducing the candidate's background, skills, and interests. Provide an overview of their professional journey, highlighting key experiences, projects, and achievements. Emphasize the candidate's strengths, such as their problem-solving abilities, creativity, and dedication to continuous learning. Incorporate examples of successful collaborations, leadership roles, and contributions to previous teams or projects. Encourage the recruiter to ask questions for further clarification or to explore specific areas of expertise. Aim to create a conversational tone that is engaging and informative, showcasing the candidate's personality and enthusiasm for their field.\n",
    "\n",
    "### Candidate Description\n",
    "{cantidate_description}\n",
    "\n",
    "### Chat History\n",
    "{chat_history}\n",
    "\n",
    "###\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"cantidate_description\", \"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(openai_api_key=st.secrets['OPENAI_API_KEY']), \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.predict(context = retriever, human_input=\"Is an pear a fruit or vegetable?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
